[mamba2]
d_model = 256
n_layer = 6
d_intermediate = 0
vocab_size = 48000
layer = Mamba2
d_state = 64
headdim = 64
ngroups = 1
rms_norm = True
pad_vocab_size_multiple = 1
device = cuda
dtype = f32
train_fn = data/wiki40b_en_min25max1024.en_48000.first1000.ids
n_examples_per_step = 4
n_steps_per_update = 16
max_lr = 0.001
min_lr = 0.00001
max_grad_norm = 1.
seed = 1234
